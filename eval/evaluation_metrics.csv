samples,accuracy,precision,recall,f1,tp,tn,fp,fn,model,scenario,reference,convert_no
1920,0.9401041666666666,0.938337801608579,0.9102730819245773,0.9240924092409241,700,1105,46,69,GPT-5 base,Human Answer,Human Answer,True
1920,0.840625,0.8890756302521008,0.6879063719115734,0.7756598240469207,529,1085,66,240,GPT-4o base,Human Answer,Human Answer,True
1920,0.9067708333333333,0.927536231884058,0.8322496749024707,0.8773132282385195,640,1101,50,129,GPT-4o FT,Human Answer,Human Answer,True
1920,0.8234375,0.7874331550802139,0.7659297789336801,0.7765326301911667,589,992,159,180,GPT-4o AP Before,Human Answer,Human Answer,True
1920,0.8375,0.8241134751773049,0.7555266579973993,0.7883310719131615,581,1027,124,188,GPT-4o AP,Human Answer,Human Answer,True
1920,0.8395833333333333,0.8618524332810047,0.7139141742522757,0.7809388335704125,549,1063,88,220,GPT-4o AP After,Human Answer,Human Answer,True
1920,0.7958333333333333,0.7696709585121603,0.6996098829648895,0.7329700272479563,538,990,161,231,Llama3.1-70B base,Human Answer,Human Answer,True
1920,0.8625,0.9566003616636528,0.6879063719115734,0.800302571860817,529,1127,24,240,Llama3.1-70B FT,Human Answer,Human Answer,True
1920,0.8161458333333333,0.7606516290726817,0.7893368010403121,0.7747287811104021,607,960,191,162,Llama3.1-70B AP Before,Human Answer,Human Answer,True
1920,0.753125,0.6577540106951871,0.7997399219765929,0.7218309859154928,615,831,320,154,Llama3.1-70B AP,Human Answer,Human Answer,True
1920,0.7098958333333333,0.613978494623656,0.7425227568270482,0.672160094173043,571,792,359,198,Llama3.1-70B AP After,Human Answer,Human Answer,True
1920,0.8041666666666667,0.8154093097913323,0.6605981794538361,0.7298850574712643,508,1036,115,261,Llama3.1-8B base,Human Answer,Human Answer,True
1920,0.8015625,0.8895582329317269,0.576072821846554,0.6992896606156275,443,1096,55,326,Llama3.1-8B FT,Human Answer,Human Answer,True
1920,0.6979166666666666,0.6070215175537939,0.6970091027308193,0.648910411622276,536,804,347,233,Llama3.1-8B AP Before,Human Answer,Human Answer,True
1920,0.575,0.48040033361134277,0.7490247074122237,0.5853658536585366,576,528,623,193,Llama3.1-8B AP,Human Answer,Human Answer,True
1920,0.5510416666666667,0.45829596412556056,0.6644993498049415,0.5424628450106158,511,547,604,258,Llama3.1-8B AP After,Human Answer,Human Answer,True
840,0.9523809523809523,0.9036144578313253,0.9336099585062241,0.9183673469387756,225,575,24,16,GPT-5 base,Human Answer – Yes/No questions,Human Answer,True
840,0.9095238095238095,0.8837209302325582,0.7883817427385892,0.8333333333333334,190,574,25,51,GPT-4o base,Human Answer – Yes/No questions,Human Answer,True
840,0.9488095238095238,0.9159663865546218,0.9045643153526971,0.9102296450939457,218,579,20,23,GPT-4o FT,Human Answer – Yes/No questions,Human Answer,True
840,0.8773809523809524,0.73,0.9087136929460581,0.8096118299445472,219,518,81,22,GPT-4o AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.9,0.7773851590106007,0.9128630705394191,0.8396946564885495,220,536,63,21,GPT-4o AP,Human Answer – Yes/No questions,Human Answer,True
840,0.9083333333333333,0.828,0.8589211618257261,0.8431771894093686,207,556,43,34,GPT-4o AP After,Human Answer – Yes/No questions,Human Answer,True
840,0.8773809523809524,0.8136363636363636,0.7427385892116183,0.7765726681127982,179,558,41,62,Llama3.1-70B base,Human Answer – Yes/No questions,Human Answer,True
840,0.9202380952380952,0.9484536082474226,0.7634854771784232,0.8459770114942528,184,589,10,57,Llama3.1-70B FT,Human Answer – Yes/No questions,Human Answer,True
840,0.8678571428571429,0.7083333333333334,0.91701244813278,0.7992766726943943,221,508,91,20,Llama3.1-70B AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.8202380952380952,0.6222826086956522,0.950207468879668,0.7520525451559934,229,460,139,12,Llama3.1-70B AP,Human Answer – Yes/No questions,Human Answer,True
840,0.7511904761904762,0.5379146919431279,0.941908713692946,0.6847662141779788,227,404,195,14,Llama3.1-70B AP After,Human Answer – Yes/No questions,Human Answer,True
840,0.8666666666666667,0.7654320987654321,0.7717842323651453,0.768595041322314,186,542,57,55,Llama3.1-8B base,Human Answer – Yes/No questions,Human Answer,True
840,0.8892857142857142,0.8775510204081632,0.7136929460580913,0.7871853546910755,172,575,24,69,Llama3.1-8B FT,Human Answer – Yes/No questions,Human Answer,True
840,0.7726190476190476,0.5644329896907216,0.9087136929460581,0.6963434022257551,219,430,169,22,Llama3.1-8B AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.6238095238095238,0.4301675977653631,0.9585062240663901,0.5938303341902313,231,293,306,10,Llama3.1-8B AP,Human Answer – Yes/No questions,Human Answer,True
840,0.5928571428571429,0.4115586690017513,0.975103734439834,0.5788177339901478,235,263,336,6,Llama3.1-8B AP After,Human Answer – Yes/No questions,Human Answer,True
