samples,accuracy,precision,recall,f1,tp,tn,fp,fn,model,scenario,reference,convert_no
1920,0.9104166666666667,0.8717310087173101,0.9102730819245773,0.89058524173028,700,1048,103,69,GPT-5 base,Human Answer,Human Answer,True
1920,0.8354166666666667,0.8743801652892562,0.6879063719115734,0.7700145560407569,529,1075,76,240,GPT-4o base,Human Answer,Human Answer,True
1920,0.9046875,0.9221902017291066,0.8322496749024707,0.8749145591250854,640,1097,54,129,GPT-4o FT,Human Answer,Human Answer,True
1920,0.8223958333333333,0.7853333333333333,0.7659297789336801,0.7755102040816326,589,990,161,180,GPT-4o AP Before,Human Answer,Human Answer,True
1920,0.8375,0.8241134751773049,0.7555266579973993,0.7883310719131615,581,1027,124,188,GPT-4o AP,Human Answer,Human Answer,True
1920,0.8375,0.8564742589703588,0.7139141742522757,0.778723404255319,549,1059,92,220,GPT-4o AP After,Human Answer,Human Answer,True
1920,0.7848958333333333,0.7472222222222222,0.6996098829648895,0.722632639355272,538,969,182,231,Llama3.1-70B base,Human Answer,Human Answer,True
1920,0.8604166666666667,0.9497307001795332,0.6879063719115734,0.7978883861236802,529,1123,28,240,Llama3.1-70B FT,Human Answer,Human Answer,True
1920,0.8125,0.7540372670807454,0.7893368010403121,0.7712833545108005,607,953,198,162,Llama3.1-70B AP Before,Human Answer,Human Answer,True
1920,0.75,0.6535600425079703,0.7997399219765929,0.7192982456140352,615,825,326,154,Llama3.1-70B AP,Human Answer,Human Answer,True
1920,0.7072916666666667,0.6106951871657754,0.7425227568270482,0.6701877934272301,571,787,364,198,Llama3.1-70B AP After,Human Answer,Human Answer,True
1920,0.7942708333333334,0.7912772585669782,0.6605981794538361,0.7200566973777462,508,1017,134,261,Llama3.1-8B base,Human Answer,Human Answer,True
1920,0.8010416666666667,0.8877755511022044,0.576072821846554,0.6987381703470031,443,1095,56,326,Llama3.1-8B FT,Human Answer,Human Answer,True
1920,0.6911458333333333,0.5982142857142857,0.6970091027308193,0.6438438438438439,536,791,360,233,Llama3.1-8B AP Before,Human Answer,Human Answer,True
1920,0.5703125,0.4768211920529801,0.7490247074122237,0.582701062215478,576,519,632,193,Llama3.1-8B AP,Human Answer,Human Answer,True
1920,0.5473958333333333,0.4554367201426025,0.6644993498049415,0.5404547858276045,511,540,611,258,Llama3.1-8B AP After,Human Answer,Human Answer,True
840,0.9523809523809523,0.9036144578313253,0.9336099585062241,0.9183673469387756,225,575,24,16,GPT-5 base,Human Answer – Yes/No questions,Human Answer,True
840,0.9095238095238095,0.8837209302325582,0.7883817427385892,0.8333333333333334,190,574,25,51,GPT-4o base,Human Answer – Yes/No questions,Human Answer,True
840,0.9488095238095238,0.9159663865546218,0.9045643153526971,0.9102296450939457,218,579,20,23,GPT-4o FT,Human Answer – Yes/No questions,Human Answer,True
840,0.8773809523809524,0.73,0.9087136929460581,0.8096118299445472,219,518,81,22,GPT-4o AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.9,0.7773851590106007,0.9128630705394191,0.8396946564885495,220,536,63,21,GPT-4o AP,Human Answer – Yes/No questions,Human Answer,True
840,0.9083333333333333,0.828,0.8589211618257261,0.8431771894093686,207,556,43,34,GPT-4o AP After,Human Answer – Yes/No questions,Human Answer,True
840,0.8773809523809524,0.8136363636363636,0.7427385892116183,0.7765726681127982,179,558,41,62,Llama3.1-70B base,Human Answer – Yes/No questions,Human Answer,True
840,0.9202380952380952,0.9484536082474226,0.7634854771784232,0.8459770114942528,184,589,10,57,Llama3.1-70B FT,Human Answer – Yes/No questions,Human Answer,True
840,0.8678571428571429,0.7083333333333334,0.91701244813278,0.7992766726943943,221,508,91,20,Llama3.1-70B AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.8202380952380952,0.6222826086956522,0.950207468879668,0.7520525451559934,229,460,139,12,Llama3.1-70B AP,Human Answer – Yes/No questions,Human Answer,True
840,0.7511904761904762,0.5379146919431279,0.941908713692946,0.6847662141779788,227,404,195,14,Llama3.1-70B AP After,Human Answer – Yes/No questions,Human Answer,True
840,0.8666666666666667,0.7654320987654321,0.7717842323651453,0.768595041322314,186,542,57,55,Llama3.1-8B base,Human Answer – Yes/No questions,Human Answer,True
840,0.8892857142857142,0.8775510204081632,0.7136929460580913,0.7871853546910755,172,575,24,69,Llama3.1-8B FT,Human Answer – Yes/No questions,Human Answer,True
840,0.7726190476190476,0.5644329896907216,0.9087136929460581,0.6963434022257551,219,430,169,22,Llama3.1-8B AP Before,Human Answer – Yes/No questions,Human Answer,True
840,0.6238095238095238,0.4301675977653631,0.9585062240663901,0.5938303341902313,231,293,306,10,Llama3.1-8B AP,Human Answer – Yes/No questions,Human Answer,True
840,0.5928571428571429,0.4115586690017513,0.975103734439834,0.5788177339901478,235,263,336,6,Llama3.1-8B AP After,Human Answer – Yes/No questions,Human Answer,True
